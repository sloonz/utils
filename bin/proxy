#!/usr/bin/python3

import os, sys, base64, re, fnmatch, traceback
import http.client, urllib.parse
import socketserver, http.server
import json

import lxml.etree

# Read config
file = os.path.expanduser("~/.config/proxy.json")
if os.path.exists(file):
	config = json.load(open(file))
else:
	config = {'user_cache': [], 'session_cache': []}

USER_AGENT = 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.2.13) Gecko/20101209 Firefox/3.6.13'

class ThreadingHTTPServer(socketserver.ThreadingMixIn, http.server.HTTPServer): pass

class ProxyError(Exception):
	def __init__(self, msg, code = 500):
		self.msg = msg
		self.code = code

def urldecode(url):
	query = urllib.parse.splitquery(url)[1]
	ret = {}
	if query:
		for elem in query.split('&'):
			try:
				name, val = elem.split('=', 1)
				ret[urllib.parse.unquote(name)] = urllib.parse.unquote(val)
			except ValueError:
				name, val = elem, None
				ret[urllib.parse.unquote(name)] = None
	return ret

def safe_read(fd, remaining):
	data = ""
	buf = True
	while buf and remaining > 0:
		buf = fd.read(remaining)
		data += buf
		remaining -= len(buf)
	return data

class CacheEntry:
	status = 200
	
	def __init__(self, fd):
		self._fd = fd
		self.headers = {}
		for line in self._fd:
			if line.strip():
				header, val = line.decode('ascii').split(':', 1)
				self.headers[header.lower()] = val.strip()
			else:
				break
	
	def read(self):
		return self._fd.read()
	
	def getheaders(self):
		return self.headers.items()
	
	def close(self):
		self._fd.close()

class CacheDir:
	def _url_to_file(self, url):
		url = urllib.parse.splittype(url)[1]
		host, path = urllib.parse.splithost(url)
		return os.path.join(self._base, host, path.replace('/', '|'))
	
	def __init__(self, base, name, is_session, patterns):
		self._base = base
		self._name = name
		self._is_session = is_session
		self._patterns = patterns
	
	def in_cache(self, url):
		f = self._url_to_file(url)
		return os.path.exists(f) and os.path.isfile(f)
	
	def put_entry(self, url, data, headers):
		headers = dict(headers)
		if any(fnmatch.fnmatch(url, pattern) for pattern in self._patterns):
			f = self._url_to_file(url)
			d = os.path.dirname(f)
			if not os.path.exists(d):
				os.mkdir(d)
			fd = open(f, 'wb+')
			for header, val in headers.items():
				fd.write(header.encode('ascii') + b":" + val.encode('ascii') + b"\n")
			fd.write(b"\n")
			fd.write(data)
			fd.close()
			return True
		return False
	
	def get_entry(self, url):
		if self.in_cache(url):
			return CacheEntry(open(self._url_to_file(url), "rb"))

class Cache:
	def __init__(self):
		self._dirs = []
		
		# Permanent cache
		try:
			from xdg.BaseDirectory import xdg_cache_home
			base = os.path.join(xdg_cache_home, "proxy")
		except ImportError:
			base = os.path.join(os.path.expanduser("~"), ".cache", "proxy")
		
		if not os.path.exists(base):
			os.makedirs(base)
		
		self._dirs.append(CacheDir(base, "USER", False, config['user_cache']))
		
		# Session cache
		base = "/tmp/proxy_cache_%d" % os.getuid()
		if not os.path.exists(base):
			os.makedirs(base)
		self._dirs.append(CacheDir(base, "SESS", True, config['session_cache']))
	
	def in_cache(self, url):
		return any(d.in_cache(url) for d in self._dirs)
	
	def put_entry(self, url, data, headers):
		for d in self._dirs:
			r = d.put_entry(url, data, headers)
			if r:
				return r
	
	def get_entry(self, url):
		for d in self._dirs:
			r = d.get_entry(url)
			if r:
				return r

class PHPProxy:
	class PHProxy_v1:
		def __init__(self, proxy):
			self.find_links_re = re.compile(re.escape(proxy._url_base).encode('utf-8') + b"\\?q=[a-zA-Z0-9%]+")
			self.proxy = proxy
		
		def get_params(self, url):
			return urllib.parse.urlencode({'q': base64.b64encode(url), 'hl': self.proxy._flags})
		
		def decode(self, url):
			return base64.b64decode(urldecode(urllib.parse.unquote(url))['q'])
		
		def remove_div(self, html):
			return html
		
		def remove_div_by_xpath(self, html, xpath):
			try:
				doc = lxml.etree.HTML(html)
				for div in doc.xpath('//body')[0].xpath(xpath):
					div.getparent().remove(div)
				return lxml.etree.tostring(doc, encoding = 'utf-8', method = 'html')
			except:
				return html

	class Glype(PHProxy_v1): 
		def __init__(self, proxy):
			self.find_links_re = re.compile(re.escape(proxy._url_base).encode('utf-8') + b"\\?u=[a-zA-Z0-9%]+&(amp;)*b=" + re.escape(proxy._flags).encode('utf-8'))
			self.proxy = proxy
		
		def get_params(self, url):
			if url.startswith('http'):
				url = url.replace('http', '', 1)
			return urllib.parse.urlencode({'u': base64.b64encode(url), 'b': self.proxy._flags})
		
		def decode(self, url):
			return "http" + base64.b64decode(urldecode(urllib.parse.unquote(url))['u'])
		
		def remove_div(self, html):
			xpath = './div[@class="top_ad" or @id="proxy_top" or ((@id="url_bar" or @id="include") and ./form)]'
			return self.remove_div_by_xpath(html, xpath)

	class PHProxy_v3(PHProxy_v1):
		def __init__(self, proxy):
			self.find_links_re = re.compile(re.escape(proxy._url_base).encode('utf-8') + b"\\?i_am_feeling_lucky=[^ \"'>]+")
			self.proxy = proxy
		
		def get_params(self, url):
			d = urldecode(url)
			if d.get('____pgfa'):
				return urllib.parse.urlencode(d)
			else:
				return urllib.parse.urlencode({'i_am_feeling_lucky': url, 'hl': self.proxy._flags})
		
		def decode(self, url):
			return urllib.parse.unquote(urldecode(url)['i_am_feeling_lucky'])
		
		def remove_div(self, html):
			return self.remove_div_by_xpath(html, './div[.//input[@name="i_am_feeling_lucky"]]')

	_cache = Cache()
	_urls = (("https://www.cship.info/poxy/index.php","2e9",PHProxy_v1), # Fast, limited siz
		("https://www.breakfence.com/browse.php","5",Glype),
		("https://www.unblockbook.biz/browse.php","5",Glype),
		("https://www.guardtunnel.com/browse.php","5",Glype),
		("https://3x6.nl/proxy/index.php","2e1",PHProxy_v3),
		("https://www.iceproxy.net/index.php","0011101000",PHProxy_v1)) # Slow, accept all sizes
	
	def __init__(self, opts, args):
		for url in PHPProxy._urls:
			if opts.get("a", "http") in url[0]:
				self._url_base, self._flags, self._version = url
				break
		
		self._version = self._version(self)
		protocol, host, self._base_path, nil, nil = urllib.parse.urlsplit(self._url_base)
		port = protocol == "https" and 443 or 80
		self._conn = http.client.HTTPSConnection(host, port = port)

	def get_phpproxy_url(self, url, complete = False):
		req = self._version.get_params(url)
		if complete:
			return '%s?%s' % (self._url_base, req)
		else:
			return '%s?%s' % (self._base_path, req)
	
	def redirect_check_errors(self, loc):
		error = urldecode(self._resp.headers.get('location')).get('error')
		if error:
			raise ProxyError("Proxy error: %s" % error, 500)
		error = urldecode(self._resp.headers.get('location')).get('e')
		if error:
			raise ProxyError("Proxy error: %s" % error, 500)
	
	def get_page(self, req):
		# Do some checks
		if not req.command in ('GET', 'HEAD', 'POST'):
			raise ProxyError('Bad method', 405)
		
		if self._cache.in_cache(req.path):
			self._resp = self._cache.get_entry(req.path)
			req.cacheflags.append("HIT")
			return
		else:
			req.cacheflags.append("MISS")
		
		# Do request
		data = None
		self._conn.putrequest(req.command == 'POST' and 'POST' or 'GET', self.get_phpproxy_url(req.path))
		self._conn.putheader('Cookie', 'flags=0011101000')
		self._conn.putheader('User-Agent', USER_AGENT)
		if req.command == 'POST' and req.headers.get('content-length'):
			data = safe_read(req.rfile, int(req.headers.get('content-length')))
			self._conn.putheader('Content-Length', len(data))
			self._conn.putheader('Content-Type', req.headers.get('content-type', 'application/x-www-form-urlencoded'))
		if req.headers.get('referer'):
			self._conn.putheader('Referer', self.get_phpproxy_url(req.headers.get('referer'), True))
		else:
			self._conn.putheader('Referer', self._url_base)
		
		self._conn.endheaders()
		if data is not None:
			self._conn.send(data)
		
		self._resp = self._conn.getresponse()

	def send_page(self, to):
		try:
			data = self._resp.read()
		except http.client.IncompleteRead:
			raise ProxyError("Read error", 500)
		if not data and self._resp.status < 300:
			raise ProxyError("Empty response", 500)
		ct = self._resp.headers.get('content-type')
		if ct and "html" in ct:
			if b"Momentary overload" in data and b"WWW.ICEPROXY.NET" in data:
				raise ProxyError("Temporary overload", 500)
			if b"The file your are attempting to download is too large" in data and b"PHProxy" in data:
				raise ProxyError("Too big file", 500)
			if b"PHProxy" in data and b'<div id="error"><p><b>' in data:
				raise ProxyError(re.findall(b'<div id="error"><p><b>(.+?)</b>', data)[0])
			def decode_link(g):
				return self._version.decode(g.group().decode('utf-8', 'replace')).encode('utf-8')
			data = self._version.find_links_re.sub(decode_link, data)
			data = self._version.remove_div(data)
		
		loc = self._resp.headers.get('location')
		if loc:
			self.redirect_check_errors(loc)
		
		to.send_response(self._resp.status)
		to.send_header('Content-Type', self._resp.headers.get('content-type', 'application/octet-stream'))
		to.send_header('Content-Length', len(data))
		if loc:
			to.send_header('Location', self._version.decode(loc))
		to.end_headers()
		if to.command != 'HEAD':
			to.wfile.write(data)
		
		if self._resp.status == 200:
			if self._cache.put_entry(to.path, data, self._resp.getheaders()):
				to.cacheflags.append("CACHE")
			else:
				to.cacheflags.append("NOCACHE")
		
		to.log_finished_request(self._resp.status)
	
	def close(self):
		self._resp.close()
		self._conn.close()

class NoProxy:
	_cache = Cache()
	
	def __init__(self, opts, args): pass
	
	def get_page(self, req):
		scheme, rest = req.path.split("://", 1)
		host, path = rest.split("/", 1)
		path = "/" + path
		if scheme == "http":
			self._conn = http.client.HTTPConnection(host)
		elif scheme == "https":
			self._conn = http.client.HTTPSConnection(host)
		else:
			raise ProxyError("Bad scheme", 400)
		
		# Do some checks
		if not req.command in ('GET', 'HEAD', 'POST'):
			raise ProxyError('Bad method', 405)
		
		if self._cache.in_cache(req.path):
			self._resp = self._cache.get_entry(req.path)
			req.cacheflags.append("HIT")
			return
		else:
			req.cacheflags.append("MISS")
		
		# Do request
		data = None
		self._conn.putrequest(req.command == 'POST' and 'POST' or 'GET', path)
		self._conn.putheader('User-Agent', req.headers.get('user-agent', USER_AGENT))
		if req.command == 'POST' and req.headers.get('content-length'):
			data = safe_read(req.rfile, int(req.headers.get('content-length')))
			self._conn.putheader('Content-Length', len(data))
			self._conn.putheader('Content-Type', req.headers.get('content-type', 'application/x-www-form-urlencoded'))
		if req.headers.get('referer'):
			self._conn.putheader('Referer', req.headers.get('referer'))
		
		self._conn.endheaders()
		if data is not None:
			self._conn.send(data)
		
		self._resp = self._conn.getresponse()

	def send_page(self, to):
		try:
			data = self._resp.read()
		except http.client.IncompleteRead:
			raise ProxyError("Read error", 500)
		
		to.send_response(self._resp.status)
		to.send_header('Content-Type', self._resp.headers.get('content-type', 'application/octet-stream'))
		to.send_header('Content-Length', len(data))
		if self._resp.headers.get('location'):
			to.send_header('Location', self._resp.headers.get('location'))
		to.end_headers()
		if to.command != 'HEAD':
			to.wfile.write(data)
		
		if self._cache.put_entry(to.path, data, self._resp.getheaders()):
			to.cacheflags.append("CACHE")
		else:
			to.cacheflags.append("NOCACHE")
		
		to.log_finished_request(self._resp.status)
	
	def close(self):
		self._resp.close()
		self._conn.close()

class RequestHandler(http.server.BaseHTTPRequestHandler):
	def __init__(self, *args):
		http.server.BaseHTTPRequestHandler.__init__(self, *args)
	
	def log_request(self, *args):
		pass
	
	def log_finished_request(self, code = "-", size = "-"):
		cacheflags = ",".join(self.cacheflags)
		self.log_message('[%s] "%s" %s', cacheflags, self.requestline, str(code))
	
	def do_GET(self):
		self.cacheflags = []
		try:
			kls = opts.get("c", "PHPProxy")
			c = eval(kls)(opts, args)
			c.get_page(self)
			c.send_page(self)
			c.close()
		except ProxyError as err:
			self.send_response(err.code, err.msg)
			self.send_header('Content-Type', 'text/plain')
			self.send_header('Content-Length', len(err.msg))
			self.end_headers()
			if self.command != 'HEAD':
				self.wfile.write(err.msg.encode('utf-8'))
			self.log_finished_request(500)
		except Exception as e:
			msg = traceback.format_exc()
			self.send_response(500, 'Internal Server Error')
			self.send_header('Content-Type', 'text/plain')
			self.send_header('Content-Length', len(msg))
			self.end_headers()
			if self.command != 'HEAD':
				self.wfile.write(msg.encode('utf-8'))
			self.log_message('[%s] "%s" 500 (%s,%s)', ",".join(self.cacheflags), self.requestline, str(type(e)), str(e))
	
	do_POST = do_GET
	do_HEAD = do_GET

if __name__ == "__main__":
	import signal
	
	def quit(*args):
		sys.exit(0)
	
	signal.signal(signal.SIGINT, quit)
	
	opts = {}
	args = []
	for arg in sys.argv[1:]:
		if arg.startswith("-") and len(arg) > 1:
			opts[arg[1]] = arg[2:] or True
		else:
			args.append(arg)
	
	RequestHandler.opts = opts
	RequestHandler.args = args
	
	httpd = ThreadingHTTPServer(('', int(opts.get("p", "8080"))), RequestHandler)
	httpd.serve_forever()
